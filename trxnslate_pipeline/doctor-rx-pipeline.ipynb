{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoctorRx: Automated Handwritten Prescription Recognition Kubeflow Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/fathur-linux/.local/lib/python3.9/site-packages (23.1.2)\n",
      "Name: kfp\n",
      "Version: 1.8.22\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: \n",
      "License: \n",
      "Location: /home/fathur-linux/miniconda3/envs/kubeflow_sdk/lib/python3.9/site-packages\n",
      "Requires: absl-py, click, cloudpickle, Deprecated, docstring-parser, fire, google-api-core, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, uritemplate, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade pip\n",
    "!pip install kfp --upgrade --user --quiet\n",
    "!pip show kfp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import kubeflow pipeline libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp \n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 1: Download data image from GCS, convert to numpy.array and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv_builder(folder_name:list, data_csv: OutputPath()):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import cv2\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # gcp creds\n",
    "    print(\"<=== initiate os env gcp creds ====>\")\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'doctorrx-387716-cdaefd627b4a.json'\n",
    "    \n",
    "    #bucket\n",
    "    print('<=== get bucket ===>')\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('doctorrx_pipeline_bucket')\n",
    "    \n",
    "    pixel_columns = ['label'] + [f'pixels_{i+1}' for i in range(60000)]\n",
    "    df = pd.DataFrame(columns=pixel_columns)\n",
    "    \n",
    "    print('<=== iterate data ===>')\n",
    "    iter = 1\n",
    "    for folder_img in folder_name:\n",
    "        blobs = bucket.list_blobs(prefix=f'data_labelled/{folder_img}/')\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('/'):\n",
    "                continue\n",
    "            # get label\n",
    "            lab_names = {'Paracetamol': 0, 'Amoxilin': 1, 'CTM': 2, 'Amlodipin': 3, 'Metformin': 4}\n",
    "            label = blob.name.split(\"/\")[1]\n",
    "            \n",
    "            # get image byte -> array\n",
    "            contents = blob.download_as_string()\n",
    "            decoded = cv2.imdecode(np.frombuffer(contents, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "            img_resize = cv2.resize(decoded, (200, 100))\n",
    "            blur = cv2.GaussianBlur(img_resize, (5,5), 0)\n",
    "            ret3,th3 = cv2.threshold(blur.astype(np.uint8),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "            to_rgb_array = np.repeat(th3[..., np.newaxis], 3, -1).reshape(1, 200, 100, 3)\n",
    "            img_ravel = to_rgb_array.ravel()\n",
    "            \n",
    "            data_concate = np.concatenate([[lab_names[str(label)]], img_ravel])\n",
    "            df.loc[len(df.index)] = data_concate\n",
    "            print(f'!{blob.name} - {iter} berhasil! ')\n",
    "            iter += 1\n",
    "        \n",
    "\n",
    "    # to_csv\n",
    "    with open(data_csv, 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_csv: InputPath(), load_data_path: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    # open data\n",
    "    with open(data_csv) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    \n",
    "    # get label and attribute    \n",
    "    X = df.iloc[:, 1:].values.reshape(-1, 200, 100, 3) / 255\n",
    "    Y = df['label']\n",
    "    \n",
    "    # createing the preprocess directory\n",
    "    os.makedirs(load_data_path, exist_ok=True)\n",
    "    \n",
    "    # save the label and features to be used by preprocess components\n",
    "    with open(f\"{load_data_path}/all_data\", 'wb') as f:\n",
    "        pickle.dump((X, Y), f)\n",
    "        \n",
    "    \n",
    "        \n",
    "    return(print('Done!'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 3: Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feautre_extraction(load_data_path: InputPath(str), feature_extraction_path: OutputPath(str)):\n",
    "        # transform to desire shape for model input -> (9, 9, 512)\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    with open(f\"{load_data_path}/all_data\", 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "        \n",
    "    X, Y = all_data\n",
    "    print(X.shape, Y.shape)\n",
    "    \n",
    "    def transform_feature(arr:np.array) -> np.array:\n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.models import Model\n",
    "        \n",
    "        model_vgg16 = VGG16(weights='imagenet', include_top=False)\n",
    "        model_vgg16 = Model(inputs=model_vgg16.inputs, outputs=model_vgg16.layers[-4].output)\n",
    "        \n",
    "        features = model_vgg16.predict(arr)\n",
    "        return features\n",
    "\n",
    "    features_train = transform_feature(arr=X)\n",
    "    \n",
    "    os.makedirs(feature_extraction_path, exist_ok = True)\n",
    "\n",
    "    with open(f\"{feature_extraction_path}/features_extract\", 'wb') as f:\n",
    "        pickle.dump((features_train, Y), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splits(feature_extraction_path:InputPath(str), train_test_split_path: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # load data\n",
    "    with open(f\"{feature_extraction_path}/features_extract\", 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "        \n",
    "    features_train, features_test = all_data\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_train, features_test, test_size=0.2, random_state=42, stratify=features_test)\n",
    "    \n",
    "    os.makedirs(train_test_split_path, exist_ok = True)\n",
    "\n",
    "    # pickle train data\n",
    "    with open(f\"{train_test_split_path}/train\", 'wb') as f:\n",
    "        pickle.dump((X_train, y_train), f)\n",
    "        \n",
    "    # pickle test data\n",
    "    with open(f\"{train_test_split_path}/test\", 'wb') as f:\n",
    "        pickle.dump((X_test, y_test), f)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 4: ML Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 4.1: CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 4.2: tf vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vgg16(batch_size:int,\n",
    "             epochs:int,\n",
    "             train_test_split_path: InputPath(str),\n",
    "             model_path: OutputPath(str)):\n",
    "    \n",
    "    import os, pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.models import Model\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{train_test_split_path}/train', 'rb') as f:\n",
    "        train_data = pd.read_pickle(f)\n",
    "        \n",
    "    X_train, y_train = train_data\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    \n",
    "\n",
    "    # model building\n",
    "    model_vgg = VGG16(weights='imagenet', include_top=False)\n",
    "    layer_input = Input(shape= (12, 6, 512))\n",
    "    \n",
    "    x = layer_input\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100,activation='relu')(x)\n",
    "    x = Dense(6,activation='softmax')(x)\n",
    "    \n",
    "\n",
    "    model_vgg = Model(layer_input, x)\n",
    "    model_vgg.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])    \n",
    "    model_vgg.summary()\n",
    "    \n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    checkpoint = ModelCheckpoint(f\"{model_path}/model.h5\", monitor='val_loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
    "    # fit the data to model\n",
    "    history = model_vgg.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[checkpoint],\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(history.history['accuracy'])\n",
    "    \n",
    "    # oading the X_test and y_test\n",
    "    with open(f'{train_test_split_path}/test', 'rb') as f:\n",
    "        test_data = pd.read_pickle(f)\n",
    "        \n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # evaluate the model and print the results\n",
    "    test_loss, test_acc = model_vgg.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "    print(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 5: confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model_path: InputPath(str),\n",
    "               train_test_split_path: InputPath(str),\n",
    "               mlpipeline_ui_metadata_path: OutputPath()) :\n",
    "    \n",
    "    import json, pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import namedtuple\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    \n",
    "    # loading the X_test and y_test\n",
    "    with open(f'{train_test_split_path}/test', 'rb') as f:\n",
    "        test_data = pd.read_pickle(f)\n",
    "        \n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    labels = {\n",
    "        0: 'Paracetamol',\n",
    "        1: 'Amoxilin',\n",
    "        2: 'CTM',\n",
    "        3: 'Amlodipin',\n",
    "        4: 'Metformin'\n",
    "    }\n",
    "    \n",
    "    # loading the model\n",
    "    model = load_model(f'{model_path}/model.h5')\n",
    "    # prediction\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    # int to label\n",
    "    y_test_label = [labels[x] for x in y_test]\n",
    "    y_pred_label = [labels[x] for x in y_pred]\n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test_label, y_pred_label)\n",
    "    print(cm)\n",
    "    vocab = list(np.unique(y_test_label))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    print(df)\n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = df[['target', 'predicted']].astype(str)\n",
    "    print(df)\n",
    "    # create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {\n",
    "                        \"name\": \"target\",\n",
    "                        \"type\": \"CATEGORY\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"predicted\",\n",
    "                        \"type\": \"CATEGORY\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"count\",\n",
    "                        \"type\": \"NUMBER\"\n",
    "                    }\n",
    "                ],\n",
    "                \"source\": df.to_csv(header=False, index=False),\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [\n",
    "                    \"Amlodipin\",\n",
    "                    \"Amoxilin\",\n",
    "                    \"CTM\",\n",
    "                    \"Metformin\",\n",
    "                    \"Paracetamol\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 6: roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(model_path: InputPath(str),\n",
    "               train_test_split_path: InputPath(str),\n",
    "               mlpipeline_ui_metadata_path: OutputPath()):\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    # loading the X_test and y_test\n",
    "    with open(f'{train_test_split_path}/test', 'rb') as f:\n",
    "        test_data = pd.read_pickle(f)\n",
    "    \n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    # loading the model\n",
    "    model = load_model(f'{model_path}/model.h5')\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred, pos_label=True)\n",
    "    # testing\n",
    "    df = pd.DataFrame({\n",
    "        'fpr':fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "    metadata = {\n",
    "            'outputs': [{\n",
    "            'type': 'roc',\n",
    "            'format': 'csv',\n",
    "            'schema': [\n",
    "                {'name': 'fpr', 'type': 'NUMBER'},\n",
    "                {'name': 'tpr', 'type': 'NUMBER'},\n",
    "                {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            \"source\": df.to_csv(header=False, index=False),\n",
    "            \"storage\": \"inline\",\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 7: saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_registry(model_path: InputPath(str),\n",
    "                   model_version: float):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    \n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'doctorrx-387716-cdaefd627b4a.json'\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket('doctorrx_pipeline_bucket')\n",
    "    blob = bucket.blob(f'models/{model_version}/model.h5')\n",
    "    \n",
    "    blob.upload_from_filename(f'{model_path}/model.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create component\n",
    "create_get_data_csv_builder = comp.create_component_from_func(get_data_csv_builder,base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_preprocessing = comp.create_component_from_func(preprocessing, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_features_extraction = comp.create_component_from_func(feautre_extraction, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_train_test_split = comp.create_component_from_func(train_test_splits, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_ft_vgg16 = comp.create_component_from_func(tf_vgg16, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_confusion_matrix = comp.create_component_from_func(confusion_matrix, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_roc_curve = comp.create_component_from_func(roc_curve, base_image='gcr.io/doctorrx-387716/doctor-rx')\n",
    "create_model_registry = comp.create_component_from_func(model_registry, base_image='gcr.io/doctorrx-387716/doctor-rx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubeflow pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dsl.pipeline(name='doctor-rx-pipeline-v:1.0',\n",
    "              description=\"Perfroms end-to-end MLOPS\")\n",
    "def doctorrx_pipeline(folder_name: list = [\"Amlodipin\", \"Paracetamol\", \"Amoxilin\", \"CTM\", \"Metformin\"],\n",
    "                      load_data_path: str = '/mnt' ,\n",
    "                      batch_size: int = 128,\n",
    "                      epochs: int = 100,\n",
    "                      train_test_split_path: str = 'testsplit',\n",
    "                      feature_extraction_path:str = 'feature_extract',\n",
    "                      model_path:str = 'model',\n",
    "                      model_version:float = 1.0):\n",
    "    \n",
    "    # create get data container\n",
    "    get_data_csv_comp = create_get_data_csv_builder(folder_name=folder_name)\n",
    "    \n",
    "    # create pre processing cotnainer\n",
    "    preprocessing_comp = create_preprocessing(get_data_csv_comp.outputs['data_csv'])\n",
    "    \n",
    "    # create features extraction\n",
    "    features_extraction_comp = create_features_extraction(preprocessing_comp.output)\n",
    "    \n",
    "    # create preprocess container\n",
    "    train_test_split_comp = create_train_test_split(features_extraction_comp.output)\n",
    "    \n",
    "    # create cnn container\n",
    "    \n",
    "    # create modelling container\n",
    "    modelling_comp = create_ft_vgg16(batch_size, epochs, train_test_split_comp.output)\n",
    "    # modelling_comp.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n",
    "       \n",
    "    # create conf container\n",
    "    conf_matrix = create_confusion_matrix(modelling_comp.output, train_test_split_comp.output)\n",
    "    # conf_matrix.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n",
    "    \n",
    "    # create roc curve\n",
    "    roc_curve = create_roc_curve(modelling_comp.output, train_test_split_comp.output)\n",
    "    # roc_curve.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n",
    "    \n",
    "    model_registry = create_model_registry(modelling_comp.output, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_name = [\"Amlodipin\", \"Paracetamol\", \"Amoxilin\", \"CTM\", \"Metformin\"]\n",
    "load_data_path = \"/mnt\"\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "train_test_split_path = \"testsplit\"\n",
    "feature_extraction_path = 'feature_extract'\n",
    "model_path = \"model\"\n",
    "version = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = doctorrx_pipeline\n",
    "experiment_name = 'doctorrx_pipeline_v2'\n",
    "run_name = pipeline_func.__name__ + 'run'\n",
    "\n",
    "arguments = {\n",
    "    \"folder_name\": folder_name,\n",
    "    \"load_data_path\": load_data_path,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"train_test_split_path\": train_test_split_path,\n",
    "    \"feature_extraction_path\": feature_extraction_path,\n",
    "    \"model_path\": model_path,\n",
    "    \"version\" : version\n",
    "}\n",
    "\n",
    "# compiler\n",
    "kfp.compiler.Compiler().compile(pipeline_func,\n",
    "                                '{}.yaml'.format(experiment_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kubeflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
